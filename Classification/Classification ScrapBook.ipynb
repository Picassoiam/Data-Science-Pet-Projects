{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "gdset=pd.read_csv('glass.csv')\ngdset.head(2)",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RI</th>\n      <th>Na</th>\n      <th>Mg</th>\n      <th>Al</th>\n      <th>Si</th>\n      <th>K</th>\n      <th>Ca</th>\n      <th>Ba</th>\n      <th>Fe</th>\n      <th>Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.52101</td>\n      <td>13.64</td>\n      <td>4.49</td>\n      <td>1.10</td>\n      <td>71.78</td>\n      <td>0.06</td>\n      <td>8.75</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.51761</td>\n      <td>13.89</td>\n      <td>3.60</td>\n      <td>1.36</td>\n      <td>72.73</td>\n      <td>0.48</td>\n      <td>7.83</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "        RI     Na    Mg    Al     Si     K    Ca   Ba   Fe  Type\n0  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0     1\n1  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0     1"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(gdset.shape)\ndisplay(gdset.head())\ndisplay(gdset.describe())",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(214, 10)\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RI</th>\n      <th>Na</th>\n      <th>Mg</th>\n      <th>Al</th>\n      <th>Si</th>\n      <th>K</th>\n      <th>Ca</th>\n      <th>Ba</th>\n      <th>Fe</th>\n      <th>Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.52101</td>\n      <td>13.64</td>\n      <td>4.49</td>\n      <td>1.10</td>\n      <td>71.78</td>\n      <td>0.06</td>\n      <td>8.75</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.51761</td>\n      <td>13.89</td>\n      <td>3.60</td>\n      <td>1.36</td>\n      <td>72.73</td>\n      <td>0.48</td>\n      <td>7.83</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.51618</td>\n      <td>13.53</td>\n      <td>3.55</td>\n      <td>1.54</td>\n      <td>72.99</td>\n      <td>0.39</td>\n      <td>7.78</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.51766</td>\n      <td>13.21</td>\n      <td>3.69</td>\n      <td>1.29</td>\n      <td>72.61</td>\n      <td>0.57</td>\n      <td>8.22</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.51742</td>\n      <td>13.27</td>\n      <td>3.62</td>\n      <td>1.24</td>\n      <td>73.08</td>\n      <td>0.55</td>\n      <td>8.07</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "        RI     Na    Mg    Al     Si     K    Ca   Ba   Fe  Type\n0  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0     1\n1  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0     1\n2  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0     1\n3  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0     1\n4  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0     1"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RI</th>\n      <th>Na</th>\n      <th>Mg</th>\n      <th>Al</th>\n      <th>Si</th>\n      <th>K</th>\n      <th>Ca</th>\n      <th>Ba</th>\n      <th>Fe</th>\n      <th>Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>214.000000</td>\n      <td>214.000000</td>\n      <td>214.000000</td>\n      <td>214.000000</td>\n      <td>214.000000</td>\n      <td>214.000000</td>\n      <td>214.000000</td>\n      <td>214.000000</td>\n      <td>214.000000</td>\n      <td>214.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.518365</td>\n      <td>13.407850</td>\n      <td>2.684533</td>\n      <td>1.444907</td>\n      <td>72.650935</td>\n      <td>0.497056</td>\n      <td>8.956963</td>\n      <td>0.175047</td>\n      <td>0.057009</td>\n      <td>2.780374</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.003037</td>\n      <td>0.816604</td>\n      <td>1.442408</td>\n      <td>0.499270</td>\n      <td>0.774546</td>\n      <td>0.652192</td>\n      <td>1.423153</td>\n      <td>0.497219</td>\n      <td>0.097439</td>\n      <td>2.103739</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.511150</td>\n      <td>10.730000</td>\n      <td>0.000000</td>\n      <td>0.290000</td>\n      <td>69.810000</td>\n      <td>0.000000</td>\n      <td>5.430000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.516523</td>\n      <td>12.907500</td>\n      <td>2.115000</td>\n      <td>1.190000</td>\n      <td>72.280000</td>\n      <td>0.122500</td>\n      <td>8.240000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.517680</td>\n      <td>13.300000</td>\n      <td>3.480000</td>\n      <td>1.360000</td>\n      <td>72.790000</td>\n      <td>0.555000</td>\n      <td>8.600000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.519157</td>\n      <td>13.825000</td>\n      <td>3.600000</td>\n      <td>1.630000</td>\n      <td>73.087500</td>\n      <td>0.610000</td>\n      <td>9.172500</td>\n      <td>0.000000</td>\n      <td>0.100000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.533930</td>\n      <td>17.380000</td>\n      <td>4.490000</td>\n      <td>3.500000</td>\n      <td>75.410000</td>\n      <td>6.210000</td>\n      <td>16.190000</td>\n      <td>3.150000</td>\n      <td>0.510000</td>\n      <td>7.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "               RI          Na          Mg          Al          Si           K  \\\ncount  214.000000  214.000000  214.000000  214.000000  214.000000  214.000000   \nmean     1.518365   13.407850    2.684533    1.444907   72.650935    0.497056   \nstd      0.003037    0.816604    1.442408    0.499270    0.774546    0.652192   \nmin      1.511150   10.730000    0.000000    0.290000   69.810000    0.000000   \n25%      1.516523   12.907500    2.115000    1.190000   72.280000    0.122500   \n50%      1.517680   13.300000    3.480000    1.360000   72.790000    0.555000   \n75%      1.519157   13.825000    3.600000    1.630000   73.087500    0.610000   \nmax      1.533930   17.380000    4.490000    3.500000   75.410000    6.210000   \n\n               Ca          Ba          Fe        Type  \ncount  214.000000  214.000000  214.000000  214.000000  \nmean     8.956963    0.175047    0.057009    2.780374  \nstd      1.423153    0.497219    0.097439    2.103739  \nmin      5.430000    0.000000    0.000000    1.000000  \n25%      8.240000    0.000000    0.000000    1.000000  \n50%      8.600000    0.000000    0.000000    2.000000  \n75%      9.172500    0.000000    0.100000    3.000000  \nmax     16.190000    3.150000    0.510000    7.000000  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_train_test(df, y_col, x_cols, ratio):\n    \"\"\" \n    This method transforms a dataframe into a train and test set, for this you need to specify:\n    1. the ratio train : test (usually 0.7)\n    2. the column with the Y_values\n    \"\"\"\n    mask = np.random.rand(len(df)) <0.7; ratio\n    df_train = df[mask]\n    df_test = df[~mask]\n       \n    Y_train = df_train[y_col].values\n    Y_test = df_test[y_col].values\n    X_train = df_train[x_cols].values\n    X_test = df_test[x_cols].values\n    return df_train, df_test, X_train, Y_train, X_test, Y_test\n \ny_col_glass = 'Type'\nx_cols_glass = list(gdset.columns.values)\nx_cols_glass.remove(y_col_glass)\n \ntrain_test_ratio = 0.7\ndf_train, df_test, X_train, Y_train, X_test, Y_test = get_train_test(gdset, y_col_glass, x_cols_glass, train_test_ratio)",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dict_classifiers = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Nearest Neighbors\": KNeighborsClassifier(),\n    \"Linear SVM\": SVC(),\n    \"Gradient Boosting Classifier\": GradientBoostingClassifier(n_estimators=1000),\n    \"Decision Tree\": tree.DecisionTreeClassifier(),\n    \"Random Forest\": RandomForestClassifier(n_estimators=1000),\n    \"Neural Net\": MLPClassifier(alpha = 1),\n    \"Naive Bayes\": GaussianNB(),\n    #\"AdaBoost\": AdaBoostClassifier(),\n    #\"QDA\": QuadraticDiscriminantAnalysis(),\n    #\"Gaussian Process\": GaussianProcessClassifier()\n}",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def batch_classify(X_train, Y_train, X_test, Y_test, no_classifiers = 5, verbose = True):\n    \"\"\"\n    This method, takes as input the X, Y matrices of the Train and Test set.\n    And fits them on all of the Classifiers specified in the dict_classifier.\n    The trained models, and accuracies are saved in a dictionary. The reason to use a dictionary\n    is because it is very easy to save the whole dictionary with the pickle module.\n    \n    Usually, the SVM, Random Forest and Gradient Boosting Classifier take quiet some time to train. \n    So it is best to train them on a smaller dataset first and \n    decide whether you want to comment them out or not based on the test accuracy score.\n    \"\"\"\n    \n    dict_models = {}\n    for classifier_name, classifier in list(dict_classifiers.items())[:no_classifiers]:\n        t_start = time.clock()\n        classifier.fit(X_train, Y_train)\n        t_end = time.clock()\n        \n        t_diff = t_end - t_start\n        train_score = classifier.score(X_train, Y_train)\n        test_score = classifier.score(X_test, Y_test)\n        \n        dict_models[classifier_name] = {'model': classifier, 'train_score': train_score, 'test_score': test_score, 'train_time': t_diff}\n        if verbose:\n            print(\"trained {c} in {f:.2f} s\".format(c=classifier_name, f=t_diff))\n    return dict_models\n \n \n \ndef display_dict_models(dict_models, sort_by='test_score'):\n    cls = [key for key in dict_models.keys()]\n    test_s = [dict_models[key]['test_score'] for key in cls]\n    training_s = [dict_models[key]['train_score'] for key in cls]\n    training_t = [dict_models[key]['train_time'] for key in cls]\n    \n    df_ = pd.DataFrame(data=np.zeros(shape=(len(cls),4)), columns = ['classifier', 'train_score', 'test_score', 'train_time'])\n    for ii in range(0,len(cls)):\n        df_.loc[ii, 'classifier'] = cls[ii]\n        df_.loc[ii, 'train_score'] = training_s[ii]\n        df_.loc[ii, 'test_score'] = test_s[ii]\n        df_.loc[ii, 'train_time'] = training_t[ii]\n    \n    display(df_.sort_values(by=sort_by, ascending=False))",
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dict_models = batch_classify(X_train, Y_train, X_test, Y_test, no_classifiers = 8)\ndisplay_dict_models(dict_models)",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n  \"this warning.\", FutureWarning)\n/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "trained Logistic Regression in 0.01 s\ntrained Nearest Neighbors in 0.00 s\ntrained Linear SVM in 0.00 s\ntrained Gradient Boosting Classifier in 3.57 s\ntrained Decision Tree in 0.00 s\ntrained Random Forest in 1.26 s\ntrained Neural Net in 0.05 s\ntrained Naive Bayes in 0.00 s\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>classifier</th>\n      <th>train_score</th>\n      <th>test_score</th>\n      <th>train_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>Random Forest</td>\n      <td>1.000000</td>\n      <td>0.792453</td>\n      <td>1.256824</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Gradient Boosting Classifier</td>\n      <td>1.000000</td>\n      <td>0.735849</td>\n      <td>3.571728</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Linear SVM</td>\n      <td>0.695652</td>\n      <td>0.716981</td>\n      <td>0.004166</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Nearest Neighbors</td>\n      <td>0.739130</td>\n      <td>0.698113</td>\n      <td>0.001564</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Decision Tree</td>\n      <td>1.000000</td>\n      <td>0.603774</td>\n      <td>0.002634</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>0.602484</td>\n      <td>0.584906</td>\n      <td>0.007304</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Naive Bayes</td>\n      <td>0.577640</td>\n      <td>0.547170</td>\n      <td>0.004250</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Neural Net</td>\n      <td>0.391304</td>\n      <td>0.339623</td>\n      <td>0.052223</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                     classifier  train_score  test_score  train_time\n5                 Random Forest     1.000000    0.792453    1.256824\n3  Gradient Boosting Classifier     1.000000    0.735849    3.571728\n2                    Linear SVM     0.695652    0.716981    0.004166\n1             Nearest Neighbors     0.739130    0.698113    0.001564\n4                 Decision Tree     1.000000    0.603774    0.002634\n0           Logistic Regression     0.602484    0.584906    0.007304\n7                   Naive Bayes     0.577640    0.547170    0.004250\n6                    Neural Net     0.391304    0.339623    0.052223"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "GDB_params = {\n    'n_estimators': [100, 500, 1000],\n    'learning_rate': [0.5, 0.1, 0.01, 0.001],\n    'criterion': ['friedman_mse', 'mse', 'mae']\n}\n \ndf_train, df_test, X_train, Y_train, X_test, Y_test = get_train_test(gdset, y_col_glass, x_cols_glass, 0.6)\n \nfor n_est in GDB_params['n_estimators']:\n    for lr in GDB_params['learning_rate']:\n        for crit in GDB_params['criterion']:\n            clf = GradientBoostingClassifier(n_estimators=n_est, \n                                             learning_rate = lr,\n                                             criterion = crit)\n            clf.fit(X_train, Y_train)\n            train_score = clf.score(X_train, Y_train)\n            test_score = clf.score(X_test, Y_test)\n            print(\"For ({}, {}, {}) - train, test score: \\t {:.5f} \\t-\\t {:.5f}\".format(n_est, lr, crit[:4], train_score, test_score))",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": "For (100, 0.5, frie) - train, test score: \t 1.00000 \t-\t 0.80000\nFor (100, 0.5, mse) - train, test score: \t 1.00000 \t-\t 0.80000\nFor (100, 0.5, mae) - train, test score: \t 0.98658 \t-\t 0.66154\nFor (100, 0.1, frie) - train, test score: \t 1.00000 \t-\t 0.80000\nFor (100, 0.1, mse) - train, test score: \t 1.00000 \t-\t 0.83077\nFor (100, 0.1, mae) - train, test score: \t 0.95973 \t-\t 0.66154\nFor (100, 0.01, frie) - train, test score: \t 0.93289 \t-\t 0.78462\nFor (100, 0.01, mse) - train, test score: \t 0.93289 \t-\t 0.78462\nFor (100, 0.01, mae) - train, test score: \t 0.73154 \t-\t 0.67692\nFor (100, 0.001, frie) - train, test score: \t 0.83893 \t-\t 0.69231\nFor (100, 0.001, mse) - train, test score: \t 0.83893 \t-\t 0.69231\nFor (100, 0.001, mae) - train, test score: \t 0.71141 \t-\t 0.66154\nFor (500, 0.5, frie) - train, test score: \t 1.00000 \t-\t 0.81538\nFor (500, 0.5, mse) - train, test score: \t 1.00000 \t-\t 0.81538\nFor (500, 0.5, mae) - train, test score: \t 0.99329 \t-\t 0.73846\nFor (500, 0.1, frie) - train, test score: \t 1.00000 \t-\t 0.83077\nFor (500, 0.1, mse) - train, test score: \t 1.00000 \t-\t 0.80000\nFor (500, 0.1, mae) - train, test score: \t 0.93289 \t-\t 0.64615\nFor (500, 0.01, frie) - train, test score: \t 1.00000 \t-\t 0.80000\nFor (500, 0.01, mse) - train, test score: \t 1.00000 \t-\t 0.80000\nFor (500, 0.01, mae) - train, test score: \t 0.92617 \t-\t 0.67692\nFor (500, 0.001, frie) - train, test score: \t 0.89262 \t-\t 0.72308\nFor (500, 0.001, mse) - train, test score: \t 0.89262 \t-\t 0.72308\nFor (500, 0.001, mae) - train, test score: \t 0.71812 \t-\t 0.67692\nFor (1000, 0.5, frie) - train, test score: \t 1.00000 \t-\t 0.80000\nFor (1000, 0.5, mse) - train, test score: \t 1.00000 \t-\t 0.80000\nFor (1000, 0.5, mae) - train, test score: \t 0.99329 \t-\t 0.70769\nFor (1000, 0.1, frie) - train, test score: \t 1.00000 \t-\t 0.78462\nFor (1000, 0.1, mse) - train, test score: \t 1.00000 \t-\t 0.78462\nFor (1000, 0.1, mae) - train, test score: \t 0.97987 \t-\t 0.64615\nFor (1000, 0.01, frie) - train, test score: \t 1.00000 \t-\t 0.80000\nFor (1000, 0.01, mse) - train, test score: \t 1.00000 \t-\t 0.80000\nFor (1000, 0.01, mae) - train, test score: \t 0.95302 \t-\t 0.67692\nFor (1000, 0.001, frie) - train, test score: \t 0.92617 \t-\t 0.78462\nFor (1000, 0.001, mse) - train, test score: \t 0.92617 \t-\t 0.78462\nFor (1000, 0.001, mae) - train, test score: \t 0.73826 \t-\t 0.66154\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}